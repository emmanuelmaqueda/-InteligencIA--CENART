{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "¿InteligenciA__Generación.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny_JA8BNWAL2"
      },
      "source": [
        "# Transferir el estilo de una obra textual a un modelo de escritura automatizada\n",
        "\n",
        "Para leer el contexto en que se generó la siguiente receta leer sobre el  [Taller de escritura computarizada](https://www.notion.so/ivanvladimir/Taller-de-escritura-computarizada-e7a7bc49b552475a92b45db6416437cd)\n",
        "\n",
        "El proceso de adaptación se lleva a cabo con los siguiente pasos\n",
        "1. Instalar las librerías de python que permiten la generación\n",
        "2. Conectarnos al google drive para guardar nuestro modelo y/o acceder la obra textual\n",
        "3. Obtener obra textual de la que se obtendrá el estilo\n",
        "4. Entrenar\n",
        "5. Generar\n",
        "\n",
        "Si ya se cuenta con un modelo entrenado, es posible brincarse el paso de entrenaiento y obtener obra (3 y 4) y pasar directamente a generar directamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXQbwUwO9FSv"
      },
      "source": [
        "## 1. Se instalan las librerias adecuadas\n",
        "\n",
        "Las siguentes librerias hechas por la empresa [Hugginface](https://huggingface.co/) permiten la adaptación (entrenamiento) y generación automatizada de textos. Las librerías son:\n",
        "1. _transformers_, librería general para manejar modelos basados en la arquitectura neruonal transformers\n",
        "2. _datasets_, librería asociada a transoformers para la manipulación de dataset/corpus de datos\n",
        "\n",
        "Al ejecutar la celda, esperar mensajes de que se descacargan archivos y se instala módulos de python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdcEOxJM9AAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08a1e9b-e0ae-4807-d490-b6e7434c7e00"
      },
      "source": [
        "import os\n",
        "import re\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 12.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 41.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 32.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 11.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 50.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.19)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 30.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 46.5 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 38.9 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, datasets\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.12.1 fsspec-2021.10.0 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwawDNpbtdbW"
      },
      "source": [
        "\n",
        "## Obtener script de adaptaición (cont. 1)\n",
        "\n",
        "La transferencia de estilo, adaptación la llevaremos a cabo usando un script que forma parte de la librería de 'transformers' para lo cual es necesario descargarlo con la siguiente celda.\n",
        "\n",
        "Esperar unmensaje de que se descarga el archivo 'run_clm.py'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT-lcDTd4HNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbce7e5f-d579-4903-f868-212194b70a12"
      },
      "source": [
        "!wget -O run_clm.py https://raw.githubusercontent.com/huggingface/transformers/v4.4.1/examples/language-modeling/run_clm.py"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-13 04:14:47--  https://raw.githubusercontent.com/huggingface/transformers/v4.4.1/examples/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18700 (18K) [text/plain]\n",
            "Saving to: ‘run_clm.py’\n",
            "\n",
            "\rrun_clm.py            0%[                    ]       0  --.-KB/s               \rrun_clm.py          100%[===================>]  18.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-10-13 04:14:47 (78.7 MB/s) - ‘run_clm.py’ saved [18700/18700]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swm8AAKopjhi"
      },
      "source": [
        "## 2. Conectar nuestro google drive\n",
        "\n",
        "La plataforma colab puede acceder a nuestro google drive, en particular esto nos sirve para guardar el modelo adaptado en google drive y volverlo a usarlo más adelante (recordar que la máquina virtual en la que se ejecua colab deja de existir depués de un tiempo de inactividad o máximo 8 horas). \n",
        "\n",
        "Para lograr la conección, ejecutar la celda, y hacer click en el link generado automáticamente; confirmar con el sistema google la conexión hasta identificar el código de acceso que se pegará en la caja de texto de la misma celda. Depupués de unos segundos aparecerá el mensaje que google drive se ha 'montado' en ua ruta \"/content/gdrive\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwfqUrYzpbMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5df5fc-df0e-4bb4-b43f-f37e45644436"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeFEoCQcU7sj"
      },
      "source": [
        "## 3. Conseguir el texto que se utilizará para adaptar\n",
        "\n",
        "Al proceso de adaptación hay que pasarla una obra que se adaptará, para lograr esto es necesario hacerla disponible dese la plataforma de colab, se pude hacer de tres forma:\n",
        "\n",
        "1. Bajarlo desde el internet directamente al ambiente de Colab\n",
        "2. Subirlo a colab a través del menu \"Files\" ubicado en la parte izquierda de la plataform\n",
        "3. Usando la conexión con google drive\n",
        "\n",
        "En general se recomienda el formato .txt en una codificiación UTF-8; también es importante tener muy claro el nombre del archivo que contiene el texto y la ruta en la que se encuentra. Si se hace a través de los pasos 3.1 y 3.2; lo más probable es que esté accesibe desde el directotio de ejecución de colab, por lo que no haría falta fijarse en la ruta.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duZ8-PDw_8k3"
      },
      "source": [
        "### 3.1 Bajarlo desde internet\n",
        "\n",
        "Usando el comando wget obtener el archivo de una URL pública con el texto. En la celda de abajo remplazar por la URL para bajar el texto recomendado. En el ejemplo se descarga El quijote en un archivo \"el_quijote.txt\".\n",
        "\n",
        "Quidado, si la celda se ejecuta varias veces con el mismo nombre de archivo a bajar se crearán varias versiones del texto con números indicando la versión.\n",
        "\n",
        "Se espera que el archivo recuperado se baje y quede visible en el directorio de ejecución de colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_jN94DeUmkm"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/jsdario/6d6c69398cb0c73111e49f1218960f79/raw/8d4fc4548d437e2a7203a5aeeace5477f598827d/el_quijote.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi-tGgtIATuJ"
      },
      "source": [
        "### 3.2 Subirlo a colab\n",
        "\n",
        "Si el archivo está de forma local en la máquina utilizada para acceder a la plataforma colab, es posible subirlo a ésta. Para este proceso se usa el menu 'Files' de las izquierda siguiendo el procedimiento después de hacer click en el botón de 'Upload'. No es necesario ejecutar ninguna celda.\n",
        "\n",
        "Se espera que el archivo subido quede visible en el directorio de ejecución de colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8dbDPNWBEJv"
      },
      "source": [
        "### 3.3 A través de google drive\n",
        "\n",
        "Si el archivo está en el servicio de nube de google drive, es posible acceder a ese archivo ya que tenemos conectado nuestro google drive a colab (ver paso 2). Para identificar la ruta y nombre del archivo, se puede usar el menú \"Files\" a la izquieda de la plataforma colab, localizar el archivo en el google drive, bajo el nombre oprimir el menu con tres punto \"...\" y escoger la opción \"Copiar path\". No es necesario ejecutar ninguna celda.\n",
        "\n",
        "Se espera haber identificado la ruta del archivo dentro de google drive que se usará como fuente de estilo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMQ_7jRCwtXf"
      },
      "source": [
        "###3.3.1 Marcas en el texto\n",
        "Para mantener la estructura del texto es recomendable indicar la longitud de cada línea y de cada párrafo, por ello la siguiente celda recupera un archivo de un ruta dentro del Google Drive personal y anexa una codificación en cada línea y en cada párrafo de dicho archivo, el modelo es entrenado con dicha codificación, la aprende y genera un texto con la estructura aprendida pero decodificada nuevamente al final de esta notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwtJPY7mPSOq"
      },
      "source": [
        "contenido= list()\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Textos-TallerCENART/Textos/frases_celebres.txt', 'r',  encoding='utf-8', errors='ignore') as archivo:\n",
        "       for linea in archivo:\n",
        "            if not linea == '\\n':\n",
        "                linea.rstrip('\\n')\n",
        "                linea2 = ' -- '+linea+' -- \\n'\n",
        "            else:\n",
        "                linea2 = '\\n'\n",
        "            \n",
        "            #print(linea2)\n",
        "            l = linea2.split('\\n --')\n",
        "            #print(l)\n",
        "            \n",
        "            if l == ['\\n']:\n",
        "                l = '*p'.join(l)\n",
        "                #print(p)\n",
        "                contenido.append('*p \\n'+ l + '*p')\n",
        "                \n",
        "            elif not l == ['\\n'] :\n",
        "                l = ' -- '.join(l)\n",
        "                contenido.append((l))\n",
        "            \n",
        "        \n",
        "\n",
        "with open('/content/gdrive/MyDrive/Textos-TallerCENART/Textos/frases_celebres_marcas.txt', 'w',  encoding='utf-8', errors='ignore') as archivo:\n",
        "    archivo.writelines(contenido)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWhcE1gBUv7b"
      },
      "source": [
        "## 4. Entrenamiento para adaptar el modelo\n",
        "\n",
        "La siguiente celda es la que adapta un modelo pre entrenado 'datificate/gpt2-small-spanish' con el estilo del texto fuente.\n",
        "\n",
        "En este paso será necesario tener claro el nombre y la ruta (para paso 3.3) del archivo textual que se usará como fuente para el estilo (ver paso 3). Esa ruta (de ser necesaria) y nombre se sustituira en la celda de abajo para la opción '--train_file' \n",
        "\n",
        "Cuidado en el formato de la celda es muy imporante mantener el símbolo '\\' al final de la línea en la opción '--train_file' y que no haya ningún símbolo o espacio después de éste.\n",
        "\n",
        "Al ejecutar la siguiente línea puede llevarse varios minutos o hasta horas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaJDLVra2yP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9439c1a4-ee05-43c4-9a95-192e416fcd24"
      },
      "source": [
        "!mkdir output\n",
        "!python run_clm.py \\\n",
        "    --model_name_or_path 'datificate/gpt2-small-spanish' \\\n",
        "    --train_file '/content/gdrive/MyDrive/Textos-TallerCENART/Textos/frases_celebres_marcas.txt'\\\n",
        "    --do_train \\\n",
        "    --block_size 128\\\n",
        "    --num_train_epochs 40 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --save_total_limit 0 \\\n",
        "    --output_dir /output/gpt2-small-spanish-adaptado"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘output’: File exists\n",
            "10/13/2021 04:15:25 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "10/13/2021 04:15:25 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/output/gpt2-small-spanish-adaptado/runs/Oct13_04-15-25_7b0437b26910,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=40.0,\n",
            "output_dir=/output/gpt2-small-spanish-adaptado,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/output/gpt2-small-spanish-adaptado,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=0,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "10/13/2021 04:15:26 - WARNING - datasets.builder -   Using custom data configuration default-14d1aec14fa30ac0\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-14d1aec14fa30ac0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "100% 1/1 [00:00<00:00, 1734.62it/s]\n",
            "100% 1/1 [00:00<00:00, 97.30it/s]\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-14d1aec14fa30ac0/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00,  8.38it/s]\n",
            "10/13/2021 04:15:26 - DEBUG - filelock -   Attempting to acquire lock 139919770631824 on /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0.lock\n",
            "10/13/2021 04:15:26 - DEBUG - filelock -   Lock 139919770631824 acquired on /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0.lock\n",
            "[INFO|file_utils.py:1664] 2021-10-13 04:15:26,572 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqts1sx6k\n",
            "Downloading: 100% 817/817 [00:00<00:00, 658kB/s]\n",
            "[INFO|file_utils.py:1668] 2021-10-13 04:15:26,916 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|file_utils.py:1676] 2021-10-13 04:15:26,917 >> creating metadata file for /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "10/13/2021 04:15:26 - DEBUG - filelock -   Attempting to release lock 139919770631824 on /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0.lock\n",
            "10/13/2021 04:15:26 - DEBUG - filelock -   Lock 139919770631824 released on /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0.lock\n",
            "[INFO|configuration_utils.py:583] 2021-10-13 04:15:26,917 >> loading configuration file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|configuration_utils.py:620] 2021-10-13 04:15:26,918 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/home/jobregon/.fastai/data/eswiki/gpt2-small-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/13/2021 04:15:27 - DEBUG - filelock -   Attempting to acquire lock 139914205904784 on /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af.lock\n",
            "10/13/2021 04:15:27 - DEBUG - filelock -   Lock 139914205904784 acquired on /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af.lock\n",
            "[INFO|file_utils.py:1664] 2021-10-13 04:15:27,267 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp02jpykll\n",
            "Downloading: 100% 620/620 [00:00<00:00, 432kB/s]\n",
            "[INFO|file_utils.py:1668] 2021-10-13 04:15:27,612 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af\n",
            "[INFO|file_utils.py:1676] 2021-10-13 04:15:27,613 >> creating metadata file for /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af\n",
            "10/13/2021 04:15:27 - DEBUG - filelock -   Attempting to release lock 139914205904784 on /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af.lock\n",
            "10/13/2021 04:15:27 - DEBUG - filelock -   Lock 139914205904784 released on /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af.lock\n",
            "[INFO|configuration_utils.py:583] 2021-10-13 04:15:27,954 >> loading configuration file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|configuration_utils.py:620] 2021-10-13 04:15:27,955 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/home/jobregon/.fastai/data/eswiki/gpt2-small-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/13/2021 04:15:28 - DEBUG - filelock -   Attempting to acquire lock 139914157843728 on /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1.lock\n",
            "10/13/2021 04:15:28 - DEBUG - filelock -   Lock 139914157843728 acquired on /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1.lock\n",
            "[INFO|file_utils.py:1664] 2021-10-13 04:15:28,661 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpv4v2x3oy\n",
            "Downloading: 100% 830k/830k [00:00<00:00, 2.10MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-10-13 04:15:29,499 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1\n",
            "[INFO|file_utils.py:1676] 2021-10-13 04:15:29,499 >> creating metadata file for /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1\n",
            "10/13/2021 04:15:29 - DEBUG - filelock -   Attempting to release lock 139914157843728 on /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1.lock\n",
            "10/13/2021 04:15:29 - DEBUG - filelock -   Lock 139914157843728 released on /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1.lock\n",
            "10/13/2021 04:15:29 - DEBUG - filelock -   Attempting to acquire lock 139914157843728 on /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275.lock\n",
            "10/13/2021 04:15:29 - DEBUG - filelock -   Lock 139914157843728 acquired on /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275.lock\n",
            "[INFO|file_utils.py:1664] 2021-10-13 04:15:29,845 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4rj0_mdo\n",
            "Downloading: 100% 496k/496k [00:00<00:00, 1.56MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-10-13 04:15:30,598 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275\n",
            "[INFO|file_utils.py:1676] 2021-10-13 04:15:30,599 >> creating metadata file for /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275\n",
            "10/13/2021 04:15:30 - DEBUG - filelock -   Attempting to release lock 139914157843728 on /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275.lock\n",
            "10/13/2021 04:15:30 - DEBUG - filelock -   Lock 139914157843728 released on /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275.lock\n",
            "10/13/2021 04:15:31 - DEBUG - filelock -   Attempting to acquire lock 139914158040656 on /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388.lock\n",
            "10/13/2021 04:15:31 - DEBUG - filelock -   Lock 139914158040656 acquired on /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388.lock\n",
            "[INFO|file_utils.py:1664] 2021-10-13 04:15:31,640 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn93i9s5h\n",
            "Downloading: 100% 387/387 [00:00<00:00, 309kB/s]\n",
            "[INFO|file_utils.py:1668] 2021-10-13 04:15:31,982 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388\n",
            "[INFO|file_utils.py:1676] 2021-10-13 04:15:31,982 >> creating metadata file for /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388\n",
            "10/13/2021 04:15:31 - DEBUG - filelock -   Attempting to release lock 139914158040656 on /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388.lock\n",
            "10/13/2021 04:15:31 - DEBUG - filelock -   Lock 139914158040656 released on /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388.lock\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-13 04:15:32,325 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/061b8f14d1843e80aa5970efb83a4455662225a104e0155a284a1042756a9ab4.cf63eaaddc5dcbc911930aebfde578c09942b5f61a62388a1a92feff96bf54f1\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-13 04:15:32,325 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/a66557dd2fdd29ffe095cb07ef1f7204e748fa974e6781d97b85f809ad441ee4.e6cb003149b6bdd12c71a4a462ace88cfde0f402fd83ffb37a685fbdc1457275\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-13 04:15:32,325 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-13 04:15:32,325 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-13 04:15:32,325 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/1e69e45b5efe4408d6823432ea8e8aad8461132ddd9af19b5d743deeb77d4626.09040d55a052267d3a9e1622f29f19d3664f0f0fcb2efd0ad7cc8b7fed2b4388\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-13 04:15:32,325 >> loading file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9b34212ef58f0e24c27d409720c5fedcde525ea3e1408b02140cc47fd83db612.e3baea54f1682a15eb9836aa6f8f2d0c51a4f724d4a96fdffb954304ff4869af\n",
            "[INFO|configuration_utils.py:583] 2021-10-13 04:15:32,667 >> loading configuration file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|configuration_utils.py:620] 2021-10-13 04:15:32,668 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/home/jobregon/.fastai/data/eswiki/gpt2-small-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:583] 2021-10-13 04:15:33,096 >> loading configuration file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4e27b79d16bb1e8c8d8dbdacc8d69e0582276f03b01f9a4b86606510ec3aa37e.da14a9f1b815edcb384b5a5116d3fd58157fa7cb5107c37050fa12bff523ccd0\n",
            "[INFO|configuration_utils.py:620] 2021-10-13 04:15:33,097 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"/home/jobregon/.fastai/data/eswiki/gpt2-small-spanish\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/13/2021 04:15:33 - DEBUG - filelock -   Attempting to acquire lock 139914205904656 on /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e.lock\n",
            "10/13/2021 04:15:33 - DEBUG - filelock -   Lock 139914205904656 acquired on /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e.lock\n",
            "[INFO|file_utils.py:1664] 2021-10-13 04:15:33,528 >> https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpl84pvx36\n",
            "Downloading: 100% 487M/487M [00:19<00:00, 25.7MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-10-13 04:15:54,004 >> storing https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e\n",
            "[INFO|file_utils.py:1676] 2021-10-13 04:15:54,004 >> creating metadata file for /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e\n",
            "10/13/2021 04:15:54 - DEBUG - filelock -   Attempting to release lock 139914205904656 on /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e.lock\n",
            "10/13/2021 04:15:54 - DEBUG - filelock -   Lock 139914205904656 released on /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e.lock\n",
            "[INFO|modeling_utils.py:1323] 2021-10-13 04:15:54,005 >> loading weights file https://huggingface.co/datificate/gpt2-small-spanish/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0ec237c70a9a09bb1de41de760c892afd94f586e9c5c6386fb19e0ac3e212bae.16626edb2f42d00b4d5dfecc39d471e29b26101d778c01541b98f3333112c54e\n",
            "[INFO|modeling_utils.py:1588] 2021-10-13 04:15:56,154 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1597] 2021-10-13 04:15:56,154 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at datificate/gpt2-small-spanish.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 1/1 [00:00<00:00, 14.08ba/s]\n",
            "100% 1/1 [00:00<00:00, 46.53ba/s]\n",
            "[INFO|trainer.py:1196] 2021-10-13 04:16:15,777 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-13 04:16:15,777 >>   Num examples = 44\n",
            "[INFO|trainer.py:1198] 2021-10-13 04:16:15,777 >>   Num Epochs = 40\n",
            "[INFO|trainer.py:1199] 2021-10-13 04:16:15,777 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1200] 2021-10-13 04:16:15,777 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1201] 2021-10-13 04:16:15,777 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-13 04:16:15,777 >>   Total optimization steps = 240\n",
            "100% 240/240 [02:07<00:00,  2.03it/s][INFO|trainer.py:1409] 2021-10-13 04:18:23,661 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 127.9147, 'train_samples_per_second': 13.759, 'train_steps_per_second': 1.876, 'train_loss': 1.340572992960612, 'epoch': 40.0}\n",
            "100% 240/240 [02:07<00:00,  1.88it/s]\n",
            "[INFO|trainer.py:1995] 2021-10-13 04:18:23,694 >> Saving model checkpoint to /output/gpt2-small-spanish-adaptado\n",
            "[INFO|configuration_utils.py:413] 2021-10-13 04:18:23,695 >> Configuration saved in /output/gpt2-small-spanish-adaptado/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-13 04:18:25,174 >> Model weights saved in /output/gpt2-small-spanish-adaptado/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-13 04:18:25,177 >> tokenizer config file saved in /output/gpt2-small-spanish-adaptado/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-13 04:18:25,178 >> Special tokens file saved in /output/gpt2-small-spanish-adaptado/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       40.0\n",
            "  train_loss               =     1.3406\n",
            "  train_runtime            = 0:02:07.91\n",
            "  train_samples            =         44\n",
            "  train_samples_per_second =     13.759\n",
            "  train_steps_per_second   =      1.876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH7XQJSeFMVK"
      },
      "source": [
        "## 4.2 Copiar modelo a google drive\n",
        "\n",
        "La siguiente celda copia el modelo a una carpta dentro de google drive llamada \"models\" si se quiere poner en otra carpeta adaptar la celda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePd-tslR3C4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a44edede-e4a2-4775-8ed1-8e49ca2cc9f9"
      },
      "source": [
        "# Moviendo el modelo al google drive\n",
        "output_dir = '/content/drive/MyDrive/models/'\n",
        "!mkdir -p '/content/drive/MyDrive/models/'\n",
        "!cp -r '/output/gpt2-small-spanish-adaptado' '{output_dir}'\n",
        "!ls '{output_dir}/gpt2-small-spanish-adaptado'"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_results.json   runs\t\t\t    trainer_state.json\n",
            "config.json\t   special_tokens_map.json  training_args.bin\n",
            "merges.txt\t   tokenizer_config.json    train_results.json\n",
            "pytorch_model.bin  tokenizer.json\t    vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7j4IcIXnW23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25bc0389-da92-4764-bcee-f4800f210c79"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTJSNNdEEMMQ"
      },
      "source": [
        "## 4.3 Más opciones al script\n",
        "\n",
        "La siguiente celda muestra todas las opciones diponibles en el texto, por si se requiere hacer de modificaciones al entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtIcsUUZEMrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99908a36-7c4b-4fca-d371-97c568ccb407"
      },
      "source": [
        "!python run_clm.py -h"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: run_clm.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
            "                  [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]\n",
            "                  [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
            "                  [--no_use_fast_tokenizer]\n",
            "                  [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n",
            "                  [--model_revision MODEL_REVISION]\n",
            "                  [--use_auth_token [USE_AUTH_TOKEN]]\n",
            "                  [--dataset_name DATASET_NAME]\n",
            "                  [--dataset_config_name DATASET_CONFIG_NAME]\n",
            "                  [--train_file TRAIN_FILE]\n",
            "                  [--validation_file VALIDATION_FILE]\n",
            "                  [--max_train_samples MAX_TRAIN_SAMPLES]\n",
            "                  [--max_val_samples MAX_VAL_SAMPLES]\n",
            "                  [--block_size BLOCK_SIZE]\n",
            "                  [--overwrite_cache [OVERWRITE_CACHE]]\n",
            "                  [--validation_split_percentage VALIDATION_SPLIT_PERCENTAGE]\n",
            "                  [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n",
            "                  --output_dir OUTPUT_DIR\n",
            "                  [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
            "                  [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
            "                  [--do_predict [DO_PREDICT]]\n",
            "                  [--evaluation_strategy {no,steps,epoch}]\n",
            "                  [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
            "                  [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "                  [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "                  [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "                  [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                  [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
            "                  [--learning_rate LEARNING_RATE]\n",
            "                  [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
            "                  [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
            "                  [--max_grad_norm MAX_GRAD_NORM]\n",
            "                  [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                  [--max_steps MAX_STEPS]\n",
            "                  [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
            "                  [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]\n",
            "                  [--log_level {debug,info,warning,error,critical,passive}]\n",
            "                  [--log_level_replica {debug,info,warning,error,critical,passive}]\n",
            "                  [--no_log_on_each_node]\n",
            "                  [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
            "                  [--logging_dir LOGGING_DIR]\n",
            "                  [--logging_strategy {no,steps,epoch}]\n",
            "                  [--logging_first_step [LOGGING_FIRST_STEP]]\n",
            "                  [--logging_steps LOGGING_STEPS]\n",
            "                  [--logging_nan_inf_filter LOGGING_NAN_INF_FILTER]\n",
            "                  [--save_strategy {no,steps,epoch}] [--save_steps SAVE_STEPS]\n",
            "                  [--save_total_limit SAVE_TOTAL_LIMIT]\n",
            "                  [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
            "                  [--no_cuda [NO_CUDA]] [--seed SEED] [--fp16 [FP16]]\n",
            "                  [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                  [--fp16_backend {auto,amp,apex}]\n",
            "                  [--fp16_full_eval [FP16_FULL_EVAL]]\n",
            "                  [--local_rank LOCAL_RANK] [--xpu_backend {mpi,ccl}]\n",
            "                  [--tpu_num_cores TPU_NUM_CORES]\n",
            "                  [--tpu_metrics_debug [TPU_METRICS_DEBUG]] [--debug DEBUG]\n",
            "                  [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
            "                  [--eval_steps EVAL_STEPS]\n",
            "                  [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "                  [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
            "                  [--disable_tqdm DISABLE_TQDM] [--no_remove_unused_columns]\n",
            "                  [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
            "                  [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
            "                  [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
            "                  [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
            "                  [--greater_is_better GREATER_IS_BETTER]\n",
            "                  [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
            "                  [--sharded_ddp SHARDED_DDP] [--deepspeed DEEPSPEED]\n",
            "                  [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
            "                  [--adafactor [ADAFACTOR]]\n",
            "                  [--group_by_length [GROUP_BY_LENGTH]]\n",
            "                  [--length_column_name LENGTH_COLUMN_NAME]\n",
            "                  [--report_to REPORT_TO [REPORT_TO ...]]\n",
            "                  [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
            "                  [--no_dataloader_pin_memory]\n",
            "                  [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
            "                  [--no_skip_memory_metrics]\n",
            "                  [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
            "                  [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
            "                  [--push_to_hub [PUSH_TO_HUB]]\n",
            "                  [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "                  [--hub_model_id HUB_MODEL_ID]\n",
            "                  [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
            "                  [--hub_token HUB_TOKEN]\n",
            "                  [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
            "                  [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
            "                  [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
            "                  [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
            "                  [--mp_parameters MP_PARAMETERS]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                        The model checkpoint for weights initialization.Don't\n",
            "                        set if you want to train a model from scratch.\n",
            "                        (default: None)\n",
            "  --model_type MODEL_TYPE\n",
            "                        If training from scratch, pass a model type from the\n",
            "                        list: gptj, rembert, roformer, bigbird_pegasus,\n",
            "                        gpt_neo, big_bird, speech_to_text_2, blenderbot-small,\n",
            "                        bert-generation, camembert, xlm-roberta, pegasus,\n",
            "                        marian, mbart, megatron-bert, bart, blenderbot,\n",
            "                        reformer, roberta, bert, openai-gpt, gpt2, transfo-xl,\n",
            "                        xlnet, xlm-prophetnet, prophetnet, xlm, ctrl (default:\n",
            "                        None)\n",
            "  --config_name CONFIG_NAME\n",
            "                        Pretrained config name or path if not the same as\n",
            "                        model_name (default: None)\n",
            "  --tokenizer_name TOKENIZER_NAME\n",
            "                        Pretrained tokenizer name or path if not the same as\n",
            "                        model_name (default: None)\n",
            "  --cache_dir CACHE_DIR\n",
            "                        Where do you want to store the pretrained models\n",
            "                        downloaded from huggingface.co (default: None)\n",
            "  --no_use_fast_tokenizer\n",
            "                        Whether to use one of the fast tokenizer (backed by\n",
            "                        the tokenizers library) or not. (default: True)\n",
            "  --use_fast_tokenizer [USE_FAST_TOKENIZER]\n",
            "                        Whether to use one of the fast tokenizer (backed by\n",
            "                        the tokenizers library) or not. (default: True)\n",
            "  --model_revision MODEL_REVISION\n",
            "                        The specific model version to use (can be a branch\n",
            "                        name, tag name or commit id). (default: main)\n",
            "  --use_auth_token [USE_AUTH_TOKEN]\n",
            "                        Will use the token generated when running\n",
            "                        `transformers-cli login` (necessary to use this script\n",
            "                        with private models). (default: False)\n",
            "  --dataset_name DATASET_NAME\n",
            "                        The name of the dataset to use (via the datasets\n",
            "                        library). (default: None)\n",
            "  --dataset_config_name DATASET_CONFIG_NAME\n",
            "                        The configuration name of the dataset to use (via the\n",
            "                        datasets library). (default: None)\n",
            "  --train_file TRAIN_FILE\n",
            "                        The input training data file (a text file). (default:\n",
            "                        None)\n",
            "  --validation_file VALIDATION_FILE\n",
            "                        An optional input evaluation data file to evaluate the\n",
            "                        perplexity on (a text file). (default: None)\n",
            "  --max_train_samples MAX_TRAIN_SAMPLES\n",
            "                        For debugging purposes or quicker training, truncate\n",
            "                        the number of training examples to this value if set.\n",
            "                        (default: None)\n",
            "  --max_val_samples MAX_VAL_SAMPLES\n",
            "                        For debugging purposes or quicker training, truncate\n",
            "                        the number of validation examples to this value if\n",
            "                        set. (default: None)\n",
            "  --block_size BLOCK_SIZE\n",
            "                        Optional input sequence length after tokenization.The\n",
            "                        training dataset will be truncated in block of this\n",
            "                        size for training.Default to the model max input\n",
            "                        length for single sentence inputs (take into account\n",
            "                        special tokens). (default: None)\n",
            "  --overwrite_cache [OVERWRITE_CACHE]\n",
            "                        Overwrite the cached training and evaluation sets\n",
            "                        (default: False)\n",
            "  --validation_split_percentage VALIDATION_SPLIT_PERCENTAGE\n",
            "                        The percentage of the train set used as validation set\n",
            "                        in case there's no validation split (default: 5)\n",
            "  --preprocessing_num_workers PREPROCESSING_NUM_WORKERS\n",
            "                        The number of processes to use for the preprocessing.\n",
            "                        (default: None)\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and\n",
            "                        checkpoints will be written. (default: None)\n",
            "  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\n",
            "                        Overwrite the content of the output directory.Use this\n",
            "                        to continue training if output_dir points to a\n",
            "                        checkpoint directory. (default: False)\n",
            "  --do_train [DO_TRAIN]\n",
            "                        Whether to run training. (default: False)\n",
            "  --do_eval [DO_EVAL]   Whether to run eval on the dev set. (default: False)\n",
            "  --do_predict [DO_PREDICT]\n",
            "                        Whether to run predictions on the test set. (default:\n",
            "                        False)\n",
            "  --evaluation_strategy {no,steps,epoch}\n",
            "                        The evaluation strategy to use. (default: no)\n",
            "  --prediction_loss_only [PREDICTION_LOSS_ONLY]\n",
            "                        When performing evaluation and predictions, only\n",
            "                        returns the loss. (default: False)\n",
            "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for training.\n",
            "                        (default: 8)\n",
            "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for evaluation.\n",
            "                        (default: 8)\n",
            "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_train_batch_size`\n",
            "                        is preferred. Batch size per GPU/TPU core/CPU for\n",
            "                        training. (default: None)\n",
            "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_eval_batch_size`\n",
            "                        is preferred.Batch size per GPU/TPU core/CPU for\n",
            "                        evaluation. (default: None)\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before\n",
            "                        performing a backward/update pass. (default: 1)\n",
            "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n",
            "                        Number of predictions steps to accumulate before\n",
            "                        moving the tensors to the CPU. (default: None)\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        The initial learning rate for AdamW. (default: 5e-05)\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "                        Weight decay for AdamW if we apply some. (default:\n",
            "                        0.0)\n",
            "  --adam_beta1 ADAM_BETA1\n",
            "                        Beta1 for AdamW optimizer (default: 0.9)\n",
            "  --adam_beta2 ADAM_BETA2\n",
            "                        Beta2 for AdamW optimizer (default: 0.999)\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon for AdamW optimizer. (default: 1e-08)\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm. (default: 1.0)\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Total number of training epochs to perform. (default:\n",
            "                        3.0)\n",
            "  --max_steps MAX_STEPS\n",
            "                        If > 0: set total number of training steps to perform.\n",
            "                        Override num_train_epochs. (default: -1)\n",
            "  --lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}\n",
            "                        The scheduler type to use. (default: linear)\n",
            "  --warmup_ratio WARMUP_RATIO\n",
            "                        Linear warmup over warmup_ratio fraction of total\n",
            "                        steps. (default: 0.0)\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        Linear warmup over warmup_steps. (default: 0)\n",
            "  --log_level {debug,info,warning,error,critical,passive}\n",
            "                        Logger log level to use on the main node. Possible\n",
            "                        choices are the log levels as strings: 'debug',\n",
            "                        'info', 'warning', 'error' and 'critical', plus a\n",
            "                        'passive' level which doesn't set anything and lets\n",
            "                        the application set the level. Defaults to 'passive'.\n",
            "                        (default: passive)\n",
            "  --log_level_replica {debug,info,warning,error,critical,passive}\n",
            "                        Logger log level to use on replica nodes. Same choices\n",
            "                        and defaults as ``log_level`` (default: passive)\n",
            "  --no_log_on_each_node\n",
            "                        When doing a multinode distributed training, whether\n",
            "                        to log once per node or just once on the main node.\n",
            "                        (default: True)\n",
            "  --log_on_each_node [LOG_ON_EACH_NODE]\n",
            "                        When doing a multinode distributed training, whether\n",
            "                        to log once per node or just once on the main node.\n",
            "                        (default: True)\n",
            "  --logging_dir LOGGING_DIR\n",
            "                        Tensorboard log dir. (default: None)\n",
            "  --logging_strategy {no,steps,epoch}\n",
            "                        The logging strategy to use. (default: steps)\n",
            "  --logging_first_step [LOGGING_FIRST_STEP]\n",
            "                        Log the first global_step (default: False)\n",
            "  --logging_steps LOGGING_STEPS\n",
            "                        Log every X updates steps. (default: 500)\n",
            "  --logging_nan_inf_filter LOGGING_NAN_INF_FILTER\n",
            "                        Filter nan and inf losses for logging. (default: True)\n",
            "  --save_strategy {no,steps,epoch}\n",
            "                        The checkpoint save strategy to use. (default: steps)\n",
            "  --save_steps SAVE_STEPS\n",
            "                        Save checkpoint every X updates steps. (default: 500)\n",
            "  --save_total_limit SAVE_TOTAL_LIMIT\n",
            "                        Limit the total amount of checkpoints.Deletes the\n",
            "                        older checkpoints in the output_dir. Default is\n",
            "                        unlimited checkpoints (default: None)\n",
            "  --save_on_each_node [SAVE_ON_EACH_NODE]\n",
            "                        When doing multi-node distributed training, whether to\n",
            "                        save models and checkpoints on each node, or only on\n",
            "                        the main one (default: False)\n",
            "  --no_cuda [NO_CUDA]   Do not use CUDA even when it is available (default:\n",
            "                        False)\n",
            "  --seed SEED           Random seed that will be set at the beginning of\n",
            "                        training. (default: 42)\n",
            "  --fp16 [FP16]         Whether to use 16-bit (mixed) precision instead of\n",
            "                        32-bit (default: False)\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "                        For fp16: Apex AMP optimization level selected in\n",
            "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
            "                        https://nvidia.github.io/apex/amp.html (default: O1)\n",
            "  --fp16_backend {auto,amp,apex}\n",
            "                        The backend to be used for mixed precision. (default:\n",
            "                        auto)\n",
            "  --fp16_full_eval [FP16_FULL_EVAL]\n",
            "                        Whether to use full 16-bit precision evaluation\n",
            "                        instead of 32-bit (default: False)\n",
            "  --local_rank LOCAL_RANK\n",
            "                        For distributed training: local_rank (default: -1)\n",
            "  --xpu_backend {mpi,ccl}\n",
            "                        The backend to be used for distributed training on\n",
            "                        Intel XPU. (default: None)\n",
            "  --tpu_num_cores TPU_NUM_CORES\n",
            "                        TPU: Number of TPU cores (automatically passed by\n",
            "                        launcher script) (default: None)\n",
            "  --tpu_metrics_debug [TPU_METRICS_DEBUG]\n",
            "                        Deprecated, the use of `--debug tpu_metrics_debug` is\n",
            "                        preferred. TPU: Whether to print debug metrics\n",
            "                        (default: False)\n",
            "  --debug DEBUG         Whether or not to enable debug mode. Current options:\n",
            "                        `underflow_overflow` (Detect underflow and overflow in\n",
            "                        activations and weights), `tpu_metrics_debug` (print\n",
            "                        debug metrics on TPU). (default: )\n",
            "  --dataloader_drop_last [DATALOADER_DROP_LAST]\n",
            "                        Drop the last incomplete batch if it is not divisible\n",
            "                        by the batch size. (default: False)\n",
            "  --eval_steps EVAL_STEPS\n",
            "                        Run an evaluation every X steps. (default: None)\n",
            "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
            "                        Number of subprocesses to use for data loading\n",
            "                        (PyTorch only). 0 means that the data will be loaded\n",
            "                        in the main process. (default: 0)\n",
            "  --past_index PAST_INDEX\n",
            "                        If >=0, uses the corresponding part of the output as\n",
            "                        the past state for next step. (default: -1)\n",
            "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\n",
            "                        wandb logging. (default: None)\n",
            "  --disable_tqdm DISABLE_TQDM\n",
            "                        Whether or not to disable the tqdm progress bars.\n",
            "                        (default: None)\n",
            "  --no_remove_unused_columns\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset. (default: True)\n",
            "  --remove_unused_columns [REMOVE_UNUSED_COLUMNS]\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset. (default: True)\n",
            "  --label_names LABEL_NAMES [LABEL_NAMES ...]\n",
            "                        The list of keys in your dictionary of inputs that\n",
            "                        correspond to the labels. (default: None)\n",
            "  --load_best_model_at_end [LOAD_BEST_MODEL_AT_END]\n",
            "                        Whether or not to load the best model found during\n",
            "                        training at the end of training. (default: False)\n",
            "  --metric_for_best_model METRIC_FOR_BEST_MODEL\n",
            "                        The metric to use to compare two different models.\n",
            "                        (default: None)\n",
            "  --greater_is_better GREATER_IS_BETTER\n",
            "                        Whether the `metric_for_best_model` should be\n",
            "                        maximized or not. (default: None)\n",
            "  --ignore_data_skip [IGNORE_DATA_SKIP]\n",
            "                        When resuming training, whether or not to skip the\n",
            "                        first epochs and batches to get to the same training\n",
            "                        data. (default: False)\n",
            "  --sharded_ddp SHARDED_DDP\n",
            "                        Whether or not to use sharded DDP training (in\n",
            "                        distributed training only). The base option should be\n",
            "                        `simple`, `zero_dp_2` or `zero_dp_3` and you can add\n",
            "                        CPU-offload to `zero_dp_2` or `zero_dp_3` like this:\n",
            "                        zero_dp_2 offload` or `zero_dp_3 offload`. You can add\n",
            "                        auto-wrap to `zero_dp_2` or with the same syntax:\n",
            "                        zero_dp_2 auto_wrap` or `zero_dp_3 auto_wrap`.\n",
            "                        (default: )\n",
            "  --deepspeed DEEPSPEED\n",
            "                        Enable deepspeed and pass the path to deepspeed json\n",
            "                        config file (e.g. ds_config.json) or an already loaded\n",
            "                        json file as a dict (default: None)\n",
            "  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\n",
            "                        The label smoothing epsilon to apply (zero means no\n",
            "                        label smoothing). (default: 0.0)\n",
            "  --adafactor [ADAFACTOR]\n",
            "                        Whether or not to replace AdamW by Adafactor.\n",
            "                        (default: False)\n",
            "  --group_by_length [GROUP_BY_LENGTH]\n",
            "                        Whether or not to group samples of roughly the same\n",
            "                        length together when batching. (default: False)\n",
            "  --length_column_name LENGTH_COLUMN_NAME\n",
            "                        Column name with precomputed lengths to use when\n",
            "                        grouping by length. (default: length)\n",
            "  --report_to REPORT_TO [REPORT_TO ...]\n",
            "                        The list of integrations to report the results and\n",
            "                        logs to. (default: None)\n",
            "  --ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS\n",
            "                        When using distributed training, the value of the flag\n",
            "                        `find_unused_parameters` passed to\n",
            "                        `DistributedDataParallel`. (default: None)\n",
            "  --no_dataloader_pin_memory\n",
            "                        Whether or not to pin memory for DataLoader. (default:\n",
            "                        True)\n",
            "  --dataloader_pin_memory [DATALOADER_PIN_MEMORY]\n",
            "                        Whether or not to pin memory for DataLoader. (default:\n",
            "                        True)\n",
            "  --no_skip_memory_metrics\n",
            "                        Whether or not to skip adding of memory profiler\n",
            "                        reports to metrics. (default: True)\n",
            "  --skip_memory_metrics [SKIP_MEMORY_METRICS]\n",
            "                        Whether or not to skip adding of memory profiler\n",
            "                        reports to metrics. (default: True)\n",
            "  --use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]\n",
            "                        Whether or not to use the legacy prediction_loop in\n",
            "                        the Trainer. (default: False)\n",
            "  --push_to_hub [PUSH_TO_HUB]\n",
            "                        Whether or not to upload the trained model to the\n",
            "                        model hub after training. (default: False)\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        The path to a folder with a valid checkpoint for your\n",
            "                        model. (default: None)\n",
            "  --hub_model_id HUB_MODEL_ID\n",
            "                        The name of the repository to keep in sync with the\n",
            "                        local `output_dir`. (default: None)\n",
            "  --hub_strategy {end,every_save,checkpoint,all_checkpoints}\n",
            "                        The hub strategy to use when `--push_to_hub` is\n",
            "                        activated. (default: every_save)\n",
            "  --hub_token HUB_TOKEN\n",
            "                        The token to use to push to the Model Hub. (default:\n",
            "                        None)\n",
            "  --gradient_checkpointing [GRADIENT_CHECKPOINTING]\n",
            "                        If True, use gradient checkpointing to save memory at\n",
            "                        the expense of slower backward pass. (default: False)\n",
            "  --push_to_hub_model_id PUSH_TO_HUB_MODEL_ID\n",
            "                        The name of the repository to which push the\n",
            "                        `Trainer`. (default: None)\n",
            "  --push_to_hub_organization PUSH_TO_HUB_ORGANIZATION\n",
            "                        The name of the organization in with to which push the\n",
            "                        `Trainer`. (default: None)\n",
            "  --push_to_hub_token PUSH_TO_HUB_TOKEN\n",
            "                        The token to use to push to the Model Hub. (default:\n",
            "                        None)\n",
            "  --mp_parameters MP_PARAMETERS\n",
            "                        Used by the SageMaker launcher to send mp-specific\n",
            "                        args. Ignored in Trainer (default: )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkhAsvah3Bv-"
      },
      "source": [
        "## 5. Generando texto\n",
        "\n",
        "La sigueintes celdas generan el texto, la primera recupera el modelo entrenado desde nuestro google drive, y la segunda hace la ejecuión. Uno puede ejecutar la segunda opción tantas veces como vea uno necesario. La opción _max_length_ controla la cantidad de texto generado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62esBZh431vF"
      },
      "source": [
        "from transformers import pipeline\n",
        "model = \"/content/drive/MyDrive/models/gpt2-small-spanish-adaptado\"\n",
        "model_text = pipeline('text-generation',model=model, tokenizer=model,)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8OseFTQ4CAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6071291-8ef1-4599-e703-73464ec30839"
      },
      "source": [
        "poem=model_text(\"Asi podemos comenzar\",max_length=500)[0]['generated_text']"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2YDLe_iGqGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8595ee55-d248-4cd5-a0e3-8b5c17356da9"
      },
      "source": [
        "poem_=re.sub(r\"--\\s*?--\",\"\\n\",poem,flags=re.DOTALL)\n",
        "poem_=re.sub(r\"\\n\\s+\",\"\\n\",poem_,flags=re.DOTALL)\n",
        "poem_=re.sub(r\"\\*p\",\"\\n\",poem_,flags=re.DOTALL)\n",
        "poem_=re.sub(r\"\\*P -- (.*?)\\s+\\n\",\"\\g<1>\\n\\n\",poem_,flags=re.DOTALL)\n",
        "poem_=re.sub(r\"-- \",\"\\n\",poem_,flags=re.DOTALL)\n",
        "print(poem_)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asi podemos comenzar con el presente y dejar atrás las fracasos, mientras proseguía la tarea, es decir, con todo buen fin, con un poco de esfuerzo. (Benjamin Franklin) \n",
            " \n",
            " \n",
            " \n",
            "La mera supervivencia no hace nada interesante. (Einstein, Albert) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en las cosas sólo son invencibles! (Phenge, Albert) \n",
            " \n",
            " \n",
            " \n",
            "La imaginación solo puede ser como el saber y elicen elciana (Adam Smith) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en las cosas solo sontona!. (Arthur Schopenhauer) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en los sueños y los sueños sin fundamento!. (Confucio ) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en los sentimientos y las conversaciones sin fundamento!. (Proverbio castellano) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en los sueños y las conversaciones sin fundamento!. (Se pezucio ) \n",
            " \n",
            " \n",
            " \n",
            "Todo lo que se imagina no es para nada que sea para nada en absoluto ¡Aportador! ¡Hay algo invencible en los sueños! (Boccaccio, Giovani) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en los sueños y los sueños sin fundamento!. (Platón ) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en los sueños y las conversaciones sin fundamento!. (Proverbio castellano ) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en los sueños y las conversaciones sin fundamento!. (Platón ) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en los sueños y los sueños sin fundamento!. (Proverbio castellano ) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en los sueños y los sueños sin fundamento!. (Confucio ) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en los sueños y los sueños sin fundamento!. (Platón ) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en los sueños y el sueño sin fundamento!. (Proverbio castellano ) \n",
            " \n",
            " \n",
            " \n",
            "¡Hay algo invencible en los sueños y el sueño sin fundamento!. (Confucio\n"
          ]
        }
      ]
    }
  ]
}